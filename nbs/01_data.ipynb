{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Module\n",
    "\n",
    "> Dataset utilities for object detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms.functional as F\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import json\n",
    "import random\n",
    "import albumentations as A\n",
    "from typing import Dict, List, Tuple, Union, Optional, Callable\n",
    "from PIL import Image\n",
    "\n",
    "from objdetect.core import box_xyxy_to_cxcywh, box_cxcywh_to_xyxy, plot_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Compose:\n",
    "    \"\"\"Composes transforms for object detection.\n",
    "    \n",
    "    Applies transforms to both images and targets (bounding boxes, labels).\n",
    "    \"\"\"\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        for t in self.transforms:\n",
    "            image, target = t(image, target)\n",
    "        return image, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Normalize:\n",
    "    \"\"\"Normalize an image with mean and standard deviation.\"\"\"\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        image = F.normalize(image, mean=self.mean, std=self.std)\n",
    "        return image, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ToTensor:\n",
    "    \"\"\"Convert image and target to tensors.\"\"\"\n",
    "    def __call__(self, image, target):\n",
    "        image = F.to_tensor(image)\n",
    "        return image, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class RandomHorizontalFlip:\n",
    "    \"\"\"Randomly horizontally flips the image with a given probability.\n",
    "    The targets are also flipped accordingly.\n",
    "    \"\"\"\n",
    "    def __init__(self, prob=0.5):\n",
    "        self.prob = prob\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        if random.random() < self.prob:\n",
    "            height, width = image.shape[-2:] if torch.is_tensor(image) else (image.height, image.width)\n",
    "            image = F.hflip(image)\n",
    "            \n",
    "            if \"boxes\" in target:\n",
    "                boxes = target[\"boxes\"]\n",
    "                # flip the x coordinates\n",
    "                boxes = boxes[:, [0, 1, 2, 3]] * torch.tensor([1, 1, 1, 1]) + torch.tensor([0, 0, 0, 0])\n",
    "                boxes[:, [0, 2]] = 1 - boxes[:, [2, 0]]  # horizontal flip\n",
    "                target[\"boxes\"] = boxes\n",
    "                \n",
    "        return image, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Resize:\n",
    "    \"\"\"Resize image to a specific size and adjust targets accordingly.\"\"\"\n",
    "    def __init__(self, size):\n",
    "        self.size = size if isinstance(size, (list, tuple)) else (size, size)\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        orig_height, orig_width = image.shape[-2:] if torch.is_tensor(image) else (image.height, image.width)\n",
    "        image = F.resize(image, self.size)\n",
    "        \n",
    "        if \"boxes\" in target and len(target[\"boxes\"]):\n",
    "            scale_x = self.size[1] / orig_width\n",
    "            scale_y = self.size[0] / orig_height\n",
    "            \n",
    "            boxes = target[\"boxes\"]\n",
    "            boxes[:, [0, 2]] *= scale_x\n",
    "            boxes[:, [1, 3]] *= scale_y\n",
    "            target[\"boxes\"] = boxes\n",
    "            \n",
    "        return image, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ObjectDetectionDataset(Dataset):\n",
    "    \"\"\"Base dataset for object detection.\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 img_files: List[str], \n",
    "                 annotations: List[Dict], \n",
    "                 transforms: Optional[Callable] = None,\n",
    "                 class_names: Optional[List[str]] = None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img_files: List of image file paths\n",
    "            annotations: List of annotation dictionaries with keys 'boxes' and 'labels'\n",
    "            transforms: Optional transforms to apply to images and targets\n",
    "            class_names: List of class names\n",
    "        \"\"\"\n",
    "        self.img_files = img_files\n",
    "        self.annotations = annotations\n",
    "        self.transforms = transforms\n",
    "        self.class_names = class_names or []\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.img_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.img_files[idx]\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        \n",
    "        annotation = self.annotations[idx]\n",
    "        target = {}\n",
    "        \n",
    "        # Convert annotations to tensors\n",
    "        if \"boxes\" in annotation and len(annotation[\"boxes\"]):\n",
    "            boxes = torch.tensor(annotation[\"boxes\"], dtype=torch.float32)\n",
    "            target[\"boxes\"] = boxes\n",
    "        else:\n",
    "            target[\"boxes\"] = torch.zeros((0, 4), dtype=torch.float32)\n",
    "            \n",
    "        if \"labels\" in annotation and len(annotation[\"labels\"]):\n",
    "            labels = torch.tensor(annotation[\"labels\"], dtype=torch.int64)\n",
    "            target[\"labels\"] = labels\n",
    "        else:\n",
    "            target[\"labels\"] = torch.zeros((0,), dtype=torch.int64)\n",
    "        \n",
    "        # Apply transforms\n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)\n",
    "            \n",
    "        return img, target\n",
    "    \n",
    "    @property\n",
    "    def num_classes(self):\n",
    "        \"\"\"Return the number of classes in the dataset.\"\"\"\n",
    "        if self.class_names:\n",
    "            return len(self.class_names)\n",
    "        else:\n",
    "            # Infer from annotations\n",
    "            all_labels = []\n",
    "            for anno in self.annotations:\n",
    "                if \"labels\" in anno and len(anno[\"labels\"]) > 0:\n",
    "                    all_labels.extend(anno[\"labels\"])\n",
    "            return max(all_labels) + 1 if all_labels else 0\n",
    "    \n",
    "    def collate_fn(self, batch):\n",
    "        \"\"\"Custom collate function for data loader.\"\"\"\n",
    "        images, targets = list(zip(*batch))\n",
    "        return images, targets\n",
    "    \n",
    "    @classmethod\n",
    "    def from_coco(cls, coco_path, splits=['train'], transforms=None):\n",
    "        \"\"\"Create dataset from COCO format annotations.\n",
    "        \n",
    "        Args:\n",
    "            coco_path: Path to COCO dataset directory\n",
    "            splits: List of splits to include ('train', 'val', etc.)\n",
    "            transforms: Optional transforms\n",
    "            \n",
    "        Returns:\n",
    "            ObjectDetectionDataset\n",
    "        \"\"\"\n",
    "        coco_path = Path(coco_path)\n",
    "        img_files = []\n",
    "        annotations = []\n",
    "        class_names = []\n",
    "        \n",
    "        for split in splits:\n",
    "            # For now we'll create a stub implementation\n",
    "            # In a real implementation, we would parse the COCO JSON files\n",
    "            pass\n",
    "        \n",
    "        return cls(img_files, annotations, transforms, class_names)\n",
    "    \n",
    "    def show_sample(self, idx, figsize=(10, 10)):\n",
    "        \"\"\"Show a sample from the dataset with annotations.\"\"\"\n",
    "        img, target = self[idx]\n",
    "        \n",
    "        # If image is a tensor, convert to PIL\n",
    "        if isinstance(img, torch.Tensor):\n",
    "            img = torchvision.transforms.ToPILImage()(img)\n",
    "            \n",
    "        boxes = target[\"boxes\"]\n",
    "        labels = target[\"labels\"]\n",
    "        class_names = self.class_names if self.class_names else None\n",
    "        \n",
    "        # Plot boxes\n",
    "        return plot_boxes(img, boxes, labels, class_names=class_names, figsize=figsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_detection_transforms(train=True, size=640, mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)):\n",
    "    \"\"\"Get transforms for object detection datasets.\n",
    "    \n",
    "    Args:\n",
    "        train: Whether to include data augmentation transforms for training\n",
    "        size: Image size for resizing\n",
    "        mean: Normalization mean\n",
    "        std: Normalization standard deviation\n",
    "        \n",
    "    Returns:\n",
    "        Compose object with transforms\n",
    "    \"\"\"\n",
    "    transforms = []\n",
    "    \n",
    "    # Add training augmentations\n",
    "    if train:\n",
    "        transforms.append(RandomHorizontalFlip())\n",
    "    \n",
    "    # Add common transforms\n",
    "    transforms.extend([\n",
    "        Resize(size),\n",
    "        ToTensor(),\n",
    "        Normalize(mean=mean, std=std),\n",
    "    ])\n",
    "    \n",
    "    return Compose(transforms)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}